# -*- coding: utf-8 -*-
"""DS_Salaries.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bjJ2O8RoRgRbXNNKIlHbIsadQb1N6yNY

**Loading the dataset**
"""

import pandas as pd

# Load the dataset
df = pd.read_csv('/content/ds_salaries.csv')

# Display the first few rows of the dataset
df.head()

"""**Data Overview**"""

# Display basic information about the dataset
df.info()

# Display summary statistics of the dataset
df.describe()

# Check for missing values
df.isnull().sum()

"""**Data Cleaning**"""

# Handle missing values (if any)

# Separate numeric and non-numeric columns
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
non_numeric_cols = df.select_dtypes(exclude=['float64', 'int64']).columns

# Fill missing values in numeric columns with the median value
df[numeric_cols] = df[numeric_cols].apply(lambda x: x.fillna(x.median()))

# For non-numeric columns, you might fill missing values with the mode or a placeholder value
# Here, we'll fill missing values with the most frequent value (mode) for simplicity
df[non_numeric_cols] = df[non_numeric_cols].apply(lambda x: x.fillna(x.mode()[0]))

# Convert categorical columns to appropriate types if necessary
df['experience_level'] = df['experience_level'].astype('category')
df['employment_type'] = df['employment_type'].astype('category')
df['job_title'] = df['job_title'].astype('category')
df['employee_residence'] = df['employee_residence'].astype('category')
df['company_location'] = df['company_location'].astype('category')
df['company_size'] = df['company_size'].astype('category')
# Display the first few rows to verify
df.head()

"""**Encoding Categorical Variables**"""

from sklearn.preprocessing import LabelEncoder

# Create a label encoder object
label_encoder = LabelEncoder()

df['experience_level_encoded'] = label_encoder.fit_transform(df['experience_level'])
df['company_size_encoded'] = label_encoder.fit_transform(df['company_size'])

# One-hot encoding for nominal categorical variables
df = pd.get_dummies(df, columns=['employment_type', 'job_title', 'employee_residence', 'company_location'], prefix=['employment', 'job', 'residence', 'location'])

# Drop original categorical columns if encoded versions are used
df.drop(columns=['experience_level', 'company_size', 'salary_currency'], inplace=True)

# Display the first few rows to verify
df.head()
# Display the first few rows to verify
#df[['experience_level', 'experience_level_encoded']].head()

"""**Separating Training and Testing Data**"""

from sklearn.model_selection import train_test_split


# Separate the features and the target variable
# Assuming 'salary_in_usd' is the target variable
X = df.drop('salary_in_usd', axis=1)
y = df['salary_in_usd']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Check the shapes of the resulting datasets
print(f'Training features shape: {X_train.shape}')
print(f'Testing features shape: {X_test.shape}')
print(f'Training labels shape: {y_train.shape}')
print(f'Testing labels shape: {y_test.shape}')

"""**Machine Learning Models Training**"""

!pip install mlflow
!pip install pyngrok

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import mlflow
import mlflow.sklearn

# Set the experiment
experiment_name = "Data_Science_Salaries_Experiment"
mlflow.set_experiment(experiment_name)

# Define a function to train and evaluate models
def train_and_evaluate(model, model_name):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Calculate evaluation metrics
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    mlflow.end_run()
    # Log the results with MLflow
    with mlflow.start_run():
        mlflow.log_param("model", model_name)
        mlflow.log_metric("rmse", rmse)
        mlflow.log_metric("mae", mae)
        mlflow.log_metric("r2", r2)
        mlflow.sklearn.log_model(model, model_name)

    print(f"Model: {model_name}")
    print(f"RMSE: {rmse}")
    print(f"MAE: {mae}")
    print(f"R²: {r2}")
    print("-" * 30)

# Train and evaluate multiple models
models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(),
    "Random Forest": RandomForestRegressor(),
    "Gradient Boosting": GradientBoostingRegressor()
}

for model_name, model in models.items():
    train_and_evaluate(model, model_name)

"""**Training and Testing Multiple models**"""

models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(),
    "Random Forest": RandomForestRegressor(),
    "Gradient Boosting": GradientBoostingRegressor()
}

results = {}
for model_name, model in models.items():
    results[model_name] = train_and_evaluate(model, model_name)

# Hyperparameter tuning for Random Forest
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

best_model = RandomForestRegressor()
grid_search = GridSearchCV(estimator=best_model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1)
grid_search.fit(X_train, y_train)

best_rf_model = grid_search.best_estimator_
print(f"Best Hyperparameters: {grid_search.best_params_}")

# Log the best model's results
with mlflow.start_run():
    mlflow.log_param("model", "Random Forest (Optimized)")
    mlflow.log_params(grid_search.best_params_)

    y_pred_best = best_rf_model.predict(X_test)

    rmse_best = mean_squared_error(y_test, y_pred_best, squared=False)
    mae_best = mean_absolute_error(y_test, y_pred_best)
    r2_best = r2_score(y_test, y_pred_best)

    mlflow.log_metric("rmse", rmse_best)
    mlflow.log_metric("mae", mae_best)
    mlflow.log_metric("r2", r2_best)
    mlflow.sklearn.log_model(best_rf_model, "best_rf_model")

    print(f"Optimized Model Metrics:")
    print(f"RMSE: {rmse_best}")
    print(f"MAE: {mae_best}")
    print(f"R²: {r2_best}")

!pip install streamlit

"""**Streamlit Application**"""

!pip install streamlit pyngrok

import pandas as pd
import numpy as np
import mlflow
import mlflow.sklearn
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.ensemble import GradientBoostingRegressor
import streamlit as st
import matplotlib.pyplot as plt


X, y = np.random.rand(100, 10), np.random.rand(100) * 100000  

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Set the experiment
experiment_name = "Data_Science_Salaries_Experiment"
mlflow.set_experiment(experiment_name)

# Hyperparameter tuning for Gradient Boosting
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

best_model = GradientBoostingRegressor()
grid_search = GridSearchCV(estimator=best_model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1)
grid_search.fit(X_train, y_train)

best_gb_model = grid_search.best_estimator_

# Log the best model's results
with mlflow.start_run():
    mlflow.log_param("model", "Gradient Boosting (Optimized)")
    mlflow.log_params(grid_search.best_params_)

    y_pred_best = best_gb_model.predict(X_test)

    rmse_best = mean_squared_error(y_test, y_pred_best, squared=False)
    mae_best = mean_absolute_error(y_test, y_pred_best)
    r2_best = r2_score(y_test, y_pred_best)

    mlflow.log_metric("rmse", rmse_best)
    mlflow.log_metric("mae", mae_best)
    mlflow.log_metric("r2", r2_best)
    mlflow.sklearn.log_model(best_gb_model, "SalaryPredictorModel")  # Use a registered model name

    print(f"Optimized Model Metrics:")
    print(f"RMSE: {rmse_best}")
    print(f"MAE: {mae_best}")
    print(f"R²: {r2_best}")

# Streamlit App
st.title("Salary Prediction App")

# Define feature names
feature_names = [
    "experience",      # Years of experience
    "education_level", # Education level (encoded)
    "job_title_Data_Scientist",
    "job_title_Data_Analyst",
    "job_title_Machine_Learning_Engineer",
    "country_US",
    "country_CA",
    "country_UK",
    "gender_Male",
    "gender_Female",
]

# Load the best model from MLflow
model = mlflow.sklearn.load_model("models:/SalaryPredictorModel/1")  # Adjust model name

# User input for features
user_input = {}
for feature in feature_names:
    user_input[feature] = st.number_input(f"Enter {feature}:", min_value=0.0)

# Convert user input to DataFrame
input_data = pd.DataFrame(user_input, index=[0])

# Predict salary
if st.button("Predict Salary"):
    prediction = model.predict(input_data)
    st.write(f"Predicted Salary: ${prediction[0]:,.2f}")

# Display model performance metrics
st.subheader("Model Performance Metrics")
metrics = {
    "RMSE": st.session_state.get("rmse", "Not available"),
    "MAE": st.session_state.get("mae", "Not available"),
    "R²": st.session_state.get("r2", "Not available")
}
for metric, value in metrics.items():
    st.write(f"{metric}: {value}")

# Feature Importance visualization
st.subheader("Feature Importance")
feature_importance = best_gb_model.feature_importances_
features = np.array(feature_names)
indices = np.argsort(feature_importance)[::-1]

plt.figure(figsize=(10, 6))
plt.title("Feature Importance")
plt.barh(features[indices], feature_importance[indices], align='center')
plt.gca().invert_yaxis()
plt.xlabel("Relative Importance")
st.pyplot(plt)

from pyngrok import ngrok

# Authenticate ngrok
ngrok.set_auth_token("2jE3TbMKTvbEBxGNsao7ZoeZnFW_2wF2P3keZD6ZRZjuUZXnx")

# Start an ngrok tunnel
mlflow_uri = ngrok.connect("http://localhost:5000")

print("MLflow tracking URI:", mlflow_uri)

!streamlit run app.py & npx ngrok http 8501

!mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns --host 0.0.0.0 --port 5000 &

import time
time.sleep(5)  # Give the server some time to start
